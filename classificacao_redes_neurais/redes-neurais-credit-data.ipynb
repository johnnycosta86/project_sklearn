{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('credit_data.csv')\n",
    "base.loc[base.age < 0, 'age'] = 40.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "previsores = base.iloc[:, 1:4].values\n",
    "classe = base.iloc[:, 4].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/johnny/anaconda3/envs/redes_neurais/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(previsores[:, 1:4])\n",
    "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "previsores = scaler.fit_transform(previsores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.72572575\nIteration 2, loss = 0.65932284\nIteration 3, loss = 0.59975093\nIteration 4, loss = 0.54844256\nIteration 5, loss = 0.50297802\nIteration 6, loss = 0.46322233\nIteration 7, loss = 0.42828751\nIteration 8, loss = 0.39693775\nIteration 9, loss = 0.36897847\nIteration 10, loss = 0.34421538\nIteration 11, loss = 0.32208196\nIteration 12, loss = 0.30231775\nIteration 13, loss = 0.28443504\nIteration 14, loss = 0.26845050\nIteration 15, loss = 0.25412931\nIteration 16, loss = 0.24115681\nIteration 17, loss = 0.22934976\nIteration 18, loss = 0.21873230\nIteration 19, loss = 0.20897356\nIteration 20, loss = 0.20007289\nIteration 21, loss = 0.19193419\nIteration 22, loss = 0.18446603\nIteration 23, loss = 0.17759066\nIteration 24, loss = 0.17131933\nIteration 25, loss = 0.16554298\nIteration 26, loss = 0.16003338\nIteration 27, loss = 0.15507299\nIteration 28, loss = 0.15037707\nIteration 29, loss = 0.14615648\nIteration 30, loss = 0.14201735\nIteration 31, loss = 0.13826688\nIteration 32, loss = 0.13477873\nIteration 33, loss = 0.13159020\nIteration 34, loss = 0.12850628\nIteration 35, loss = 0.12570110\nIteration 36, loss = 0.12304476\n",
      "Iteration 37, loss = 0.12043750\nIteration 38, loss = 0.11810722\nIteration 39, loss = 0.11591391\nIteration 40, loss = 0.11371921\nIteration 41, loss = 0.11167320\nIteration 42, loss = 0.10970134\nIteration 43, loss = 0.10791625\nIteration 44, loss = 0.10603800\nIteration 45, loss = 0.10429885\nIteration 46, loss = 0.10273214\nIteration 47, loss = 0.10125028\nIteration 48, loss = 0.09963724\nIteration 49, loss = 0.09815557\nIteration 50, loss = 0.09686329\nIteration 51, loss = 0.09546330\nIteration 52, loss = 0.09420313\nIteration 53, loss = 0.09296526\nIteration 54, loss = 0.09167856\nIteration 55, loss = 0.09063782\nIteration 56, loss = 0.08943403\nIteration 57, loss = 0.08835014\nIteration 58, loss = 0.08728205\nIteration 59, loss = 0.08626298\nIteration 60, loss = 0.08524669\nIteration 61, loss = 0.08430521\nIteration 62, loss = 0.08337050\nIteration 63, loss = 0.08244821\nIteration 64, loss = 0.08155624\nIteration 65, loss = 0.08067399\nIteration 66, loss = 0.07980086\nIteration 67, loss = 0.07891427\nIteration 68, loss = 0.07809169\nIteration 69, loss = 0.07740447\nIteration 70, loss = 0.07649425\nIteration 71, loss = 0.07578088\nIteration 72, loss = 0.07501815\nIteration 73, loss = 0.07427899\nIteration 74, loss = 0.07354722\nIteration 75, loss = 0.07280381\n",
      "Iteration 76, loss = 0.07212256\nIteration 77, loss = 0.07143280\nIteration 78, loss = 0.07081890\nIteration 79, loss = 0.07006765\nIteration 80, loss = 0.06935111\nIteration 81, loss = 0.06866927\nIteration 82, loss = 0.06796881\nIteration 83, loss = 0.06733952\nIteration 84, loss = 0.06680570\nIteration 85, loss = 0.06613256\nIteration 86, loss = 0.06545825\nIteration 87, loss = 0.06486098\nIteration 88, loss = 0.06429790\nIteration 89, loss = 0.06369978\nIteration 90, loss = 0.06313887\nIteration 91, loss = 0.06255511\nIteration 92, loss = 0.06207484\nIteration 93, loss = 0.06149188\nIteration 94, loss = 0.06096975\nIteration 95, loss = 0.06052191\nIteration 96, loss = 0.05994600\nIteration 97, loss = 0.05945902\nIteration 98, loss = 0.05898203\nIteration 99, loss = 0.05850758\nIteration 100, loss = 0.05808573\nIteration 101, loss = 0.05760551\nIteration 102, loss = 0.05708483\nIteration 103, loss = 0.05665165\nIteration 104, loss = 0.05623100\nIteration 105, loss = 0.05572309\nIteration 106, loss = 0.05530778\nIteration 107, loss = 0.05487778\nIteration 108, loss = 0.05442679\nIteration 109, loss = 0.05391340\nIteration 110, loss = 0.05351143\nIteration 111, loss = 0.05314259\nIteration 112, loss = 0.05273548\nIteration 113, loss = 0.05225366\nIteration 114, loss = 0.05189181\nIteration 115, loss = 0.05152531\nIteration 116, loss = 0.05111186\nIteration 117, loss = 0.05062527\nIteration 118, loss = 0.05034902\n",
      "Iteration 119, loss = 0.04990130\nIteration 120, loss = 0.04956818\nIteration 121, loss = 0.04914590\nIteration 122, loss = 0.04882095\nIteration 123, loss = 0.04850254\nIteration 124, loss = 0.04812922\nIteration 125, loss = 0.04777650\nIteration 126, loss = 0.04744574\nIteration 127, loss = 0.04708285\nIteration 128, loss = 0.04680049\nIteration 129, loss = 0.04645439\nIteration 130, loss = 0.04611629\nIteration 131, loss = 0.04580351\nIteration 132, loss = 0.04551430\nIteration 133, loss = 0.04526244\nIteration 134, loss = 0.04498787\nIteration 135, loss = 0.04456559\nIteration 136, loss = 0.04428185\nIteration 137, loss = 0.04404057\nIteration 138, loss = 0.04371260\nIteration 139, loss = 0.04342579\nIteration 140, loss = 0.04313742\nIteration 141, loss = 0.04285245\nIteration 142, loss = 0.04271220\nIteration 143, loss = 0.04235834\nIteration 144, loss = 0.04199723\nIteration 145, loss = 0.04176022\nIteration 146, loss = 0.04149580\nIteration 147, loss = 0.04120213\nIteration 148, loss = 0.04097412\nIteration 149, loss = 0.04074095\nIteration 150, loss = 0.04045306\nIteration 151, loss = 0.04030518\nIteration 152, loss = 0.03996006\nIteration 153, loss = 0.03971220\nIteration 154, loss = 0.03949894\nIteration 155, loss = 0.03920086\nIteration 156, loss = 0.03902233\n",
      "Iteration 157, loss = 0.03876452\nIteration 158, loss = 0.03857181\nIteration 159, loss = 0.03831456\nIteration 160, loss = 0.03806521\nIteration 161, loss = 0.03783696\nIteration 162, loss = 0.03762789\nIteration 163, loss = 0.03738682\nIteration 164, loss = 0.03720722\nIteration 165, loss = 0.03693950\nIteration 166, loss = 0.03679317\nIteration 167, loss = 0.03651204\nIteration 168, loss = 0.03636017\nIteration 169, loss = 0.03611162\nIteration 170, loss = 0.03598607\nIteration 171, loss = 0.03574128\nIteration 172, loss = 0.03551938\nIteration 173, loss = 0.03532281\nIteration 174, loss = 0.03520486\nIteration 175, loss = 0.03492199\nIteration 176, loss = 0.03474096\nIteration 177, loss = 0.03452564\nIteration 178, loss = 0.03434707\nIteration 179, loss = 0.03421621\nIteration 180, loss = 0.03407876\nIteration 181, loss = 0.03376595\nIteration 182, loss = 0.03359995\nIteration 183, loss = 0.03341438\nIteration 184, loss = 0.03325131\nIteration 185, loss = 0.03307562\nIteration 186, loss = 0.03292335\nIteration 187, loss = 0.03274072\nIteration 188, loss = 0.03262987\nIteration 189, loss = 0.03242812\nIteration 190, loss = 0.03226132\nIteration 191, loss = 0.03208829\nIteration 192, loss = 0.03190593\n",
      "Iteration 193, loss = 0.03179299\nIteration 194, loss = 0.03160471\nIteration 195, loss = 0.03148595\nIteration 196, loss = 0.03125115\nIteration 197, loss = 0.03117316\nIteration 198, loss = 0.03107675\nIteration 199, loss = 0.03089914\nIteration 200, loss = 0.03074033\nIteration 201, loss = 0.03051530\nIteration 202, loss = 0.03039953\nIteration 203, loss = 0.03019315\nIteration 204, loss = 0.03006323\nIteration 205, loss = 0.02995143\nIteration 206, loss = 0.02978930\nIteration 207, loss = 0.02963706\nIteration 208, loss = 0.02953091\nIteration 209, loss = 0.02938945\nIteration 210, loss = 0.02924776\nIteration 211, loss = 0.02913291\nIteration 212, loss = 0.02899971\nIteration 213, loss = 0.02883255\nIteration 214, loss = 0.02871335\nIteration 215, loss = 0.02859530\nIteration 216, loss = 0.02849352\nIteration 217, loss = 0.02831522\nIteration 218, loss = 0.02818729\nIteration 219, loss = 0.02807071\nIteration 220, loss = 0.02796795\nIteration 221, loss = 0.02791787\nIteration 222, loss = 0.02775531\nIteration 223, loss = 0.02763827\nIteration 224, loss = 0.02750838\nIteration 225, loss = 0.02736711\nIteration 226, loss = 0.02733392\nIteration 227, loss = 0.02712550\nIteration 228, loss = 0.02699410\nIteration 229, loss = 0.02684443\nIteration 230, loss = 0.02677575\nIteration 231, loss = 0.02663394\nIteration 232, loss = 0.02654416\n",
      "Iteration 233, loss = 0.02642118\nIteration 234, loss = 0.02623547\nIteration 235, loss = 0.02624202\nIteration 236, loss = 0.02611662\nIteration 237, loss = 0.02596273\nIteration 238, loss = 0.02583246\nIteration 239, loss = 0.02575106\nIteration 240, loss = 0.02567726\nIteration 241, loss = 0.02549833\nIteration 242, loss = 0.02541954\nIteration 243, loss = 0.02540027\nIteration 244, loss = 0.02519088\nIteration 245, loss = 0.02520969\nIteration 246, loss = 0.02501252\nIteration 247, loss = 0.02492380\nIteration 248, loss = 0.02487369\nIteration 249, loss = 0.02470641\nIteration 250, loss = 0.02462719\nIteration 251, loss = 0.02451904\nIteration 252, loss = 0.02440890\nIteration 253, loss = 0.02429847\nIteration 254, loss = 0.02422970\nIteration 255, loss = 0.02416130\nIteration 256, loss = 0.02405873\nIteration 257, loss = 0.02393452\nIteration 258, loss = 0.02389167\nIteration 259, loss = 0.02378053\nIteration 260, loss = 0.02368058\nIteration 261, loss = 0.02355323\nIteration 262, loss = 0.02351706\nIteration 263, loss = 0.02342663\nIteration 264, loss = 0.02331184\nIteration 265, loss = 0.02325262\nIteration 266, loss = 0.02324707\nIteration 267, loss = 0.02305658\nIteration 268, loss = 0.02300652",
      "\nIteration 269, loss = 0.02289927\nIteration 270, loss = 0.02290224\nIteration 271, loss = 0.02275547\nIteration 272, loss = 0.02263027\nIteration 273, loss = 0.02254107\nIteration 274, loss = 0.02252075\nIteration 275, loss = 0.02241394\nIteration 276, loss = 0.02228489\nIteration 277, loss = 0.02220702\nIteration 278, loss = 0.02220477\nIteration 279, loss = 0.02211902\nIteration 280, loss = 0.02201800\nIteration 281, loss = 0.02194598\nIteration 282, loss = 0.02179785\nIteration 283, loss = 0.02179311\nIteration 284, loss = 0.02171658\nIteration 285, loss = 0.02161617\nIteration 286, loss = 0.02150948\nIteration 287, loss = 0.02141620\nIteration 288, loss = 0.02138425\nIteration 289, loss = 0.02137241\nIteration 290, loss = 0.02130449\nIteration 291, loss = 0.02113502\nIteration 292, loss = 0.02109144\nIteration 293, loss = 0.02106983\nIteration 294, loss = 0.02098884\nIteration 295, loss = 0.02087038\nIteration 296, loss = 0.02086589\nIteration 297, loss = 0.02071103\nIteration 298, loss = 0.02070711\nIteration 299, loss = 0.02063265\nIteration 300, loss = 0.02056126\nIteration 301, loss = 0.02045805\nIteration 302, loss = 0.02039650\nIteration 303, loss = 0.02031182\nIteration 304, loss = 0.02021005\nIteration 305, loss = 0.02016028\nIteration 306, loss = 0.02010999\nIteration 307, loss = 0.02009229\nIteration 308, loss = 0.01997275\nIteration 309, loss = 0.01987402\nIteration 310, loss = 0.01985964\nIteration 311, loss = 0.01978229\nIteration 312, loss = 0.01974190\nIteration 313, loss = 0.01963387\nIteration 314, loss = 0.01958283\n",
      "Iteration 315, loss = 0.01949919\nIteration 316, loss = 0.01947967\nIteration 317, loss = 0.01940448\nIteration 318, loss = 0.01932959\nIteration 319, loss = 0.01926160\nIteration 320, loss = 0.01921775\nIteration 321, loss = 0.01912370\nIteration 322, loss = 0.01914717\nIteration 323, loss = 0.01904327\nIteration 324, loss = 0.01908208\nIteration 325, loss = 0.01894440\nIteration 326, loss = 0.01887386\nIteration 327, loss = 0.01880840\nIteration 328, loss = 0.01875242\nIteration 329, loss = 0.01869613\nIteration 330, loss = 0.01864994\nIteration 331, loss = 0.01859490\nIteration 332, loss = 0.01858884\nIteration 333, loss = 0.01847208\nIteration 334, loss = 0.01845270\nIteration 335, loss = 0.01831149\nIteration 336, loss = 0.01825313\nIteration 337, loss = 0.01822326\nIteration 338, loss = 0.01832487\nIteration 339, loss = 0.01817976\nIteration 340, loss = 0.01801071\nIteration 341, loss = 0.01807987\nIteration 342, loss = 0.01809052\nIteration 343, loss = 0.01786683\nIteration 344, loss = 0.01781326\nIteration 345, loss = 0.01781416\nIteration 346, loss = 0.01773655\nIteration 347, loss = 0.01774681\nIteration 348, loss = 0.01765918\nIteration 349, loss = 0.01758512\nIteration 350, loss = 0.01752207\nIteration 351, loss = 0.01746643\n",
      "Iteration 352, loss = 0.01738486\nIteration 353, loss = 0.01734140\nIteration 354, loss = 0.01727690\nIteration 355, loss = 0.01730668\nIteration 356, loss = 0.01728690\nIteration 357, loss = 0.01716328\nIteration 358, loss = 0.01711169\nIteration 359, loss = 0.01706276\nIteration 360, loss = 0.01706049\nIteration 361, loss = 0.01699885\nIteration 362, loss = 0.01697978\nIteration 363, loss = 0.01692717\nIteration 364, loss = 0.01685642\nIteration 365, loss = 0.01677634\nIteration 366, loss = 0.01675885\nIteration 367, loss = 0.01674438\nIteration 368, loss = 0.01671017\nIteration 369, loss = 0.01655186\nIteration 370, loss = 0.01657913\nIteration 371, loss = 0.01648910\nIteration 372, loss = 0.01641366\nIteration 373, loss = 0.01642210\nIteration 374, loss = 0.01637352\nIteration 375, loss = 0.01629290\nIteration 376, loss = 0.01632659\nIteration 377, loss = 0.01627146\nIteration 378, loss = 0.01615731\nIteration 379, loss = 0.01623148\nIteration 380, loss = 0.01614752\nIteration 381, loss = 0.01607427\nIteration 382, loss = 0.01606343\nIteration 383, loss = 0.01588237\nIteration 384, loss = 0.01589843\nIteration 385, loss = 0.01595365\nIteration 386, loss = 0.01586768\nIteration 387, loss = 0.01581922\nIteration 388, loss = 0.01573511\nIteration 389, loss = 0.01563961\nIteration 390, loss = 0.01570607\nIteration 391, loss = 0.01564998\nIteration 392, loss = 0.01563446\n",
      "Iteration 393, loss = 0.01558249\nIteration 394, loss = 0.01548926\nIteration 395, loss = 0.01541224\nIteration 396, loss = 0.01538052\nIteration 397, loss = 0.01534931\nIteration 398, loss = 0.01530124\nIteration 399, loss = 0.01527892\nIteration 400, loss = 0.01523177\nIteration 401, loss = 0.01518788\nIteration 402, loss = 0.01513323\nIteration 403, loss = 0.01508975\nIteration 404, loss = 0.01501373\nIteration 405, loss = 0.01504686\nIteration 406, loss = 0.01498061\nIteration 407, loss = 0.01492434\nIteration 408, loss = 0.01486680\nIteration 409, loss = 0.01490018\nIteration 410, loss = 0.01486605\nIteration 411, loss = 0.01478636\nIteration 412, loss = 0.01473409\nIteration 413, loss = 0.01477234\nIteration 414, loss = 0.01466846\nIteration 415, loss = 0.01459806\nIteration 416, loss = 0.01460837\nIteration 417, loss = 0.01465206\nIteration 418, loss = 0.01453440\nIteration 419, loss = 0.01450578\nIteration 420, loss = 0.01444867\nIteration 421, loss = 0.01438043\nIteration 422, loss = 0.01436412\nIteration 423, loss = 0.01432070\nIteration 424, loss = 0.01423442\nIteration 425, loss = 0.01424405\nIteration 426, loss = 0.01419489\n",
      "Iteration 427, loss = 0.01419526\nIteration 428, loss = 0.01414207\nIteration 429, loss = 0.01405578\nIteration 430, loss = 0.01405151\nIteration 431, loss = 0.01400946\nIteration 432, loss = 0.01400746\nIteration 433, loss = 0.01392194\nIteration 434, loss = 0.01406367\nIteration 435, loss = 0.01386662\nIteration 436, loss = 0.01385889\nIteration 437, loss = 0.01383968\nIteration 438, loss = 0.01379218\nIteration 439, loss = 0.01372966\nIteration 440, loss = 0.01371123\nIteration 441, loss = 0.01382659\nIteration 442, loss = 0.01362826\nIteration 443, loss = 0.01365138\nIteration 444, loss = 0.01359500\nIteration 445, loss = 0.01354744\nIteration 446, loss = 0.01360258\nIteration 447, loss = 0.01353039\nIteration 448, loss = 0.01342307\nIteration 449, loss = 0.01348278\nIteration 450, loss = 0.01345525\nIteration 451, loss = 0.01336146\nIteration 452, loss = 0.01330903\nIteration 453, loss = 0.01326197\nIteration 454, loss = 0.01328694\nIteration 455, loss = 0.01329278\nIteration 456, loss = 0.01321957\nIteration 457, loss = 0.01312480\nIteration 458, loss = 0.01310641\nIteration 459, loss = 0.01312461\nIteration 460, loss = 0.01308639\nIteration 461, loss = 0.01299321\nIteration 462, loss = 0.01299247\nIteration 463, loss = 0.01306400\nIteration 464, loss = 0.01297375\nIteration 465, loss = 0.01289544\nIteration 466, loss = 0.01290822\nIteration 467, loss = 0.01282557\n",
      "Iteration 468, loss = 0.01279995\nIteration 469, loss = 0.01283831\nIteration 470, loss = 0.01272712\nIteration 471, loss = 0.01268936\nIteration 472, loss = 0.01273106\nIteration 473, loss = 0.01268031\nIteration 474, loss = 0.01263938\nIteration 475, loss = 0.01256477\nIteration 476, loss = 0.01262090\nIteration 477, loss = 0.01256644\nIteration 478, loss = 0.01252426\nIteration 479, loss = 0.01249194\nIteration 480, loss = 0.01247915\nIteration 481, loss = 0.01250010\nIteration 482, loss = 0.01248509\nIteration 483, loss = 0.01237778\nIteration 484, loss = 0.01239288\nIteration 485, loss = 0.01234457\nIteration 486, loss = 0.01224498\nIteration 487, loss = 0.01226288\nIteration 488, loss = 0.01221860\nIteration 489, loss = 0.01217612\nIteration 490, loss = 0.01219737\nIteration 491, loss = 0.01211416\nIteration 492, loss = 0.01210195\nIteration 493, loss = 0.01215803\nIteration 494, loss = 0.01210717\nIteration 495, loss = 0.01203154\nIteration 496, loss = 0.01201438\nIteration 497, loss = 0.01194511\nIteration 498, loss = 0.01192390\nIteration 499, loss = 0.01187158\nIteration 500, loss = 0.01192202\nIteration 501, loss = 0.01188142\nIteration 502, loss = 0.01187981\nIteration 503, loss = 0.01179621\nIteration 504, loss = 0.01191219\n",
      "Iteration 505, loss = 0.01174022\nIteration 506, loss = 0.01169759\nIteration 507, loss = 0.01175600\nIteration 508, loss = 0.01169640\nIteration 509, loss = 0.01168971\nIteration 510, loss = 0.01162853\nIteration 511, loss = 0.01156329\nIteration 512, loss = 0.01158945\nIteration 513, loss = 0.01155748\nIteration 514, loss = 0.01152365\nIteration 515, loss = 0.01147559\nIteration 516, loss = 0.01144887\nIteration 517, loss = 0.01139122\nIteration 518, loss = 0.01148945\nIteration 519, loss = 0.01147981\nIteration 520, loss = 0.01144818\nIteration 521, loss = 0.01138631\nIteration 522, loss = 0.01129264\nIteration 523, loss = 0.01133025\nIteration 524, loss = 0.01126905\nIteration 525, loss = 0.01123596\nIteration 526, loss = 0.01120350\nIteration 527, loss = 0.01127828\nIteration 528, loss = 0.01123406\nIteration 529, loss = 0.01112481\nIteration 530, loss = 0.01124416\nIteration 531, loss = 0.01110997\nIteration 532, loss = 0.01109024\nIteration 533, loss = 0.01104545\nIteration 534, loss = 0.01101525\nIteration 535, loss = 0.01100175\nIteration 536, loss = 0.01097221\nIteration 537, loss = 0.01092031\nIteration 538, loss = 0.01097523\nIteration 539, loss = 0.01087510\nIteration 540, loss = 0.01094500\nIteration 541, loss = 0.01086343\nIteration 542, loss = 0.01085028\nIteration 543, loss = 0.01083620\nIteration 544, loss = 0.01080358\nIteration 545, loss = 0.01079281\nIteration 546, loss = 0.01071941\n",
      "Iteration 547, loss = 0.01072446\nIteration 548, loss = 0.01073116\nIteration 549, loss = 0.01067485\nIteration 550, loss = 0.01061126\nIteration 551, loss = 0.01063094\nIteration 552, loss = 0.01055675\nIteration 553, loss = 0.01058115\nIteration 554, loss = 0.01058553\nIteration 555, loss = 0.01054269\nIteration 556, loss = 0.01052205\nIteration 557, loss = 0.01044323\nIteration 558, loss = 0.01042942\nIteration 559, loss = 0.01047332\nIteration 560, loss = 0.01040428\nIteration 561, loss = 0.01041593\nIteration 562, loss = 0.01032426\nIteration 563, loss = 0.01030856\nIteration 564, loss = 0.01038691\nIteration 565, loss = 0.01030047\nIteration 566, loss = 0.01030989\nIteration 567, loss = 0.01027289\nIteration 568, loss = 0.01022283\nIteration 569, loss = 0.01020934\nIteration 570, loss = 0.01022943\nIteration 571, loss = 0.01016280\nIteration 572, loss = 0.01015021\nIteration 573, loss = 0.01014706\nIteration 574, loss = 0.01016055\nIteration 575, loss = 0.01007236\nIteration 576, loss = 0.01007052\nIteration 577, loss = 0.01010659\nIteration 578, loss = 0.01003773\nIteration 579, loss = 0.00998903\nIteration 580, loss = 0.01003124\nIteration 581, loss = 0.01001144\nIteration 582, loss = 0.00992382\nIteration 583, loss = 0.00995828\nIteration 584, loss = 0.00995034\nIteration 585, loss = 0.00989882\n",
      "Iteration 586, loss = 0.00999922\nIteration 587, loss = 0.00984524\nIteration 588, loss = 0.00991035\nIteration 589, loss = 0.00984600\nIteration 590, loss = 0.00982514\nIteration 591, loss = 0.00977595\nIteration 592, loss = 0.00973196\nIteration 593, loss = 0.00973500\nIteration 594, loss = 0.00967867\nIteration 595, loss = 0.00970505\nIteration 596, loss = 0.00972847\nIteration 597, loss = 0.00962213\nIteration 598, loss = 0.00993621\nIteration 599, loss = 0.00961036\nIteration 600, loss = 0.00965237\nIteration 601, loss = 0.00958016\nIteration 602, loss = 0.00954069\nIteration 603, loss = 0.00952647\nIteration 604, loss = 0.00952049\nIteration 605, loss = 0.00949003\nIteration 606, loss = 0.00950182\nIteration 607, loss = 0.00945619\nIteration 608, loss = 0.00948123\nIteration 609, loss = 0.00945394\nIteration 610, loss = 0.00942191\nIteration 611, loss = 0.00942870\nIteration 612, loss = 0.00935387\nIteration 613, loss = 0.00934100\nIteration 614, loss = 0.00930955\nIteration 615, loss = 0.00930952\nIteration 616, loss = 0.00928667\nIteration 617, loss = 0.00930434\nIteration 618, loss = 0.00925810\nIteration 619, loss = 0.00924797\nIteration 620, loss = 0.00925940\nIteration 621, loss = 0.00929245\nIteration 622, loss = 0.00925071\nIteration 623, loss = 0.00917328\nIteration 624, loss = 0.00916063\nIteration 625, loss = 0.00918074\nIteration 626, loss = 0.00914976\nIteration 627, loss = 0.00912603\nIteration 628, loss = 0.00912718\n",
      "Iteration 629, loss = 0.00911020\nIteration 630, loss = 0.00906065\nIteration 631, loss = 0.00904917\nIteration 632, loss = 0.00905970\nIteration 633, loss = 0.00903926\nIteration 634, loss = 0.00901600\nIteration 635, loss = 0.00894669\nIteration 636, loss = 0.00895880\nIteration 637, loss = 0.00892667\nIteration 638, loss = 0.00892173\nIteration 639, loss = 0.00890858\nIteration 640, loss = 0.00889660\nIteration 641, loss = 0.00883440\nIteration 642, loss = 0.00885630\nIteration 643, loss = 0.00882559\nIteration 644, loss = 0.00880949\nIteration 645, loss = 0.00882139\nIteration 646, loss = 0.00883284\nIteration 647, loss = 0.00871705\nIteration 648, loss = 0.00878617\nIteration 649, loss = 0.00874681\nIteration 650, loss = 0.00874075\nIteration 651, loss = 0.00873428\nIteration 652, loss = 0.00863158\nIteration 653, loss = 0.00875496\nIteration 654, loss = 0.00871144\nIteration 655, loss = 0.00865748\nIteration 656, loss = 0.00863302\nIteration 657, loss = 0.00864323\nIteration 658, loss = 0.00853549\nIteration 659, loss = 0.00858926\nIteration 660, loss = 0.00856683\nIteration 661, loss = 0.00856350\nIteration 662, loss = 0.00853128\n",
      "Iteration 663, loss = 0.00854515\nIteration 664, loss = 0.00851093\nIteration 665, loss = 0.00850950\nIteration 666, loss = 0.00847072\nIteration 667, loss = 0.00844032\nIteration 668, loss = 0.00839962\nIteration 669, loss = 0.00841368\nIteration 670, loss = 0.00841160\nIteration 671, loss = 0.00838075\nIteration 672, loss = 0.00834138\nIteration 673, loss = 0.00834893\nIteration 674, loss = 0.00833379\nIteration 675, loss = 0.00827015\nIteration 676, loss = 0.00862109\nIteration 677, loss = 0.00828757\nIteration 678, loss = 0.00819612\nIteration 679, loss = 0.00850201\nIteration 680, loss = 0.00832247\nIteration 681, loss = 0.00827829\nIteration 682, loss = 0.00839231\nIteration 683, loss = 0.00816526\nIteration 684, loss = 0.00814074\nIteration 685, loss = 0.00827763\nIteration 686, loss = 0.00816239\nIteration 687, loss = 0.00811787\nIteration 688, loss = 0.00813080\nIteration 689, loss = 0.00811233\nIteration 690, loss = 0.00806120\nIteration 691, loss = 0.00807090\nIteration 692, loss = 0.00810337\nIteration 693, loss = 0.00805730\nIteration 694, loss = 0.00806039\nIteration 695, loss = 0.00808693\nIteration 696, loss = 0.00799228\nIteration 697, loss = 0.00798212\nIteration 698, loss = 0.00792708\nIteration 699, loss = 0.00800709\nIteration 700, loss = 0.00797535\nIteration 701, loss = 0.00805210\nIteration 702, loss = 0.00810246\nIteration 703, loss = 0.00807585\nIteration 704, loss = 0.00791578\nIteration 705, loss = 0.00787400\n",
      "Iteration 706, loss = 0.00783767\nIteration 707, loss = 0.00788940\nIteration 708, loss = 0.00784131\nIteration 709, loss = 0.00779826\nIteration 710, loss = 0.00778814\nIteration 711, loss = 0.00777478\nIteration 712, loss = 0.00775222\nIteration 713, loss = 0.00777713\nIteration 714, loss = 0.00775102\nIteration 715, loss = 0.00780730\nIteration 716, loss = 0.00771855\nIteration 717, loss = 0.00769855\nIteration 718, loss = 0.00767830\nIteration 719, loss = 0.00766582\nIteration 720, loss = 0.00765925\nIteration 721, loss = 0.00768028\nIteration 722, loss = 0.00769980\nIteration 723, loss = 0.00762727\nIteration 724, loss = 0.00763962\nIteration 725, loss = 0.00756669\nIteration 726, loss = 0.00758234\nIteration 727, loss = 0.00763125\nIteration 728, loss = 0.00764467\nIteration 729, loss = 0.00753069\nIteration 730, loss = 0.00751606\nIteration 731, loss = 0.00766100\nIteration 732, loss = 0.00750958\nIteration 733, loss = 0.00752378\nIteration 734, loss = 0.00750663\nIteration 735, loss = 0.00755531\nIteration 736, loss = 0.00750500\nIteration 737, loss = 0.00742940\nIteration 738, loss = 0.00744460\nIteration 739, loss = 0.00749018\n",
      "Iteration 740, loss = 0.00742320\nIteration 741, loss = 0.00740605\nIteration 742, loss = 0.00739680\nIteration 743, loss = 0.00736548\nIteration 744, loss = 0.00738945\nIteration 745, loss = 0.00735942\nIteration 746, loss = 0.00732044\nIteration 747, loss = 0.00728296\nIteration 748, loss = 0.00735759\nIteration 749, loss = 0.00728478\nIteration 750, loss = 0.00725465\nIteration 751, loss = 0.00725081\nIteration 752, loss = 0.00725603\nIteration 753, loss = 0.00728100\nIteration 754, loss = 0.00721135\nIteration 755, loss = 0.00722373\nIteration 756, loss = 0.00729130\nIteration 757, loss = 0.00722585\nIteration 758, loss = 0.00714735\nIteration 759, loss = 0.00719910\nIteration 760, loss = 0.00730910\nIteration 761, loss = 0.00720600\nIteration 762, loss = 0.00718825\nIteration 763, loss = 0.00710810\nIteration 764, loss = 0.00706991\nIteration 765, loss = 0.00712132\nIteration 766, loss = 0.00712308\nIteration 767, loss = 0.00711418\nIteration 768, loss = 0.00704137\nIteration 769, loss = 0.00704869\nIteration 770, loss = 0.00705578\nIteration 771, loss = 0.00703475\nIteration 772, loss = 0.00703687\nIteration 773, loss = 0.00700894\nIteration 774, loss = 0.00714333\nIteration 775, loss = 0.00704968\nIteration 776, loss = 0.00700604\nIteration 777, loss = 0.00701532\nIteration 778, loss = 0.00701298\nIteration 779, loss = 0.00690887\n",
      "Iteration 780, loss = 0.00692339\nIteration 781, loss = 0.00689246\nIteration 782, loss = 0.00689158\nIteration 783, loss = 0.00690979\nIteration 784, loss = 0.00694263\nIteration 785, loss = 0.00688057\nIteration 786, loss = 0.00688130\nIteration 787, loss = 0.00685648\nIteration 788, loss = 0.00677471\nIteration 789, loss = 0.00682891\nIteration 790, loss = 0.00681536\nIteration 791, loss = 0.00677458\nIteration 792, loss = 0.00685205\nIteration 793, loss = 0.00676792\nIteration 794, loss = 0.00677036\nIteration 795, loss = 0.00673522\nIteration 796, loss = 0.00675551\nIteration 797, loss = 0.00676281\nIteration 798, loss = 0.00672812\nIteration 799, loss = 0.00668516\nIteration 800, loss = 0.00677253\nIteration 801, loss = 0.00680461\nIteration 802, loss = 0.00674191\nIteration 803, loss = 0.00668476\nIteration 804, loss = 0.00663852\nIteration 805, loss = 0.00662386\nIteration 806, loss = 0.00661795\nIteration 807, loss = 0.00660540\nIteration 808, loss = 0.00660474\nIteration 809, loss = 0.00665372\nIteration 810, loss = 0.00659783\nIteration 811, loss = 0.00668957\nIteration 812, loss = 0.00656171\nIteration 813, loss = 0.00664494\nIteration 814, loss = 0.00653194\nIteration 815, loss = 0.00654459\nIteration 816, loss = 0.00656009\nIteration 817, loss = 0.00649043\nIteration 818, loss = 0.00649809\n",
      "Iteration 819, loss = 0.00655152\nIteration 820, loss = 0.00656177\nIteration 821, loss = 0.00651351\nIteration 822, loss = 0.00655818\nIteration 823, loss = 0.00646180\nIteration 824, loss = 0.00643426\nIteration 825, loss = 0.00641771\nIteration 826, loss = 0.00642893\nIteration 827, loss = 0.00640678\nIteration 828, loss = 0.00637672\nIteration 829, loss = 0.00641220\nIteration 830, loss = 0.00647032\nIteration 831, loss = 0.00637503\nIteration 832, loss = 0.00637382\nIteration 833, loss = 0.00634732\nIteration 834, loss = 0.00630415\nIteration 835, loss = 0.00635093\nIteration 836, loss = 0.00630627\nIteration 837, loss = 0.00629613\nIteration 838, loss = 0.00637415\nIteration 839, loss = 0.00630274\nIteration 840, loss = 0.00635283\nIteration 841, loss = 0.00632584\nIteration 842, loss = 0.00629531\nIteration 843, loss = 0.00628516\nIteration 844, loss = 0.00624193\nIteration 845, loss = 0.00633703\nIteration 846, loss = 0.00620575\nIteration 847, loss = 0.00619992\nIteration 848, loss = 0.00620228\nIteration 849, loss = 0.00617537\nIteration 850, loss = 0.00626205\nIteration 851, loss = 0.00613460\nIteration 852, loss = 0.00613466\nIteration 853, loss = 0.00616350\nIteration 854, loss = 0.00612482\nIteration 855, loss = 0.00611848\nIteration 856, loss = 0.00613355\nIteration 857, loss = 0.00609185\nIteration 858, loss = 0.00617370\nIteration 859, loss = 0.00607654\nIteration 860, loss = 0.00609432\nIteration 861, loss = 0.00604421\nIteration 862, loss = 0.00609209\nIteration 863, loss = 0.00607253\n",
      "Iteration 864, loss = 0.00604479\nIteration 865, loss = 0.00604477\nIteration 866, loss = 0.00605232\nIteration 867, loss = 0.00600989\nIteration 868, loss = 0.00597398\nIteration 869, loss = 0.00599266\nIteration 870, loss = 0.00600591\nIteration 871, loss = 0.00592339\nIteration 872, loss = 0.00599341\nIteration 873, loss = 0.00595468\nIteration 874, loss = 0.00599048\nIteration 875, loss = 0.00594418\nIteration 876, loss = 0.00590181\nIteration 877, loss = 0.00593029\nIteration 878, loss = 0.00591516\nIteration 879, loss = 0.00590577\nIteration 880, loss = 0.00590091\nIteration 881, loss = 0.00591337\nIteration 882, loss = 0.00587809\nIteration 883, loss = 0.00583426\nIteration 884, loss = 0.00589001\nIteration 885, loss = 0.00582755\nIteration 886, loss = 0.00586494\nIteration 887, loss = 0.00586187\nIteration 888, loss = 0.00589118\nIteration 889, loss = 0.00581277\nIteration 890, loss = 0.00576343\nIteration 891, loss = 0.00581406\nIteration 892, loss = 0.00576270\nIteration 893, loss = 0.00573324\nIteration 894, loss = 0.00576896\nIteration 895, loss = 0.00578791\nIteration 896, loss = 0.00576729\nIteration 897, loss = 0.00572247\nIteration 898, loss = 0.00581500\nIteration 899, loss = 0.00574658\nIteration 900, loss = 0.00571799\nIteration 901, loss = 0.00583056\n",
      "Iteration 902, loss = 0.00565548\nIteration 903, loss = 0.00571690\nIteration 904, loss = 0.00574784\nIteration 905, loss = 0.00564909\nIteration 906, loss = 0.00566326\nIteration 907, loss = 0.00571671\nIteration 908, loss = 0.00564603\nIteration 909, loss = 0.00559406\nIteration 910, loss = 0.00568437\nIteration 911, loss = 0.00565074\nIteration 912, loss = 0.00565851\nIteration 913, loss = 0.00562854\nIteration 914, loss = 0.00557447\nIteration 915, loss = 0.00559349\nIteration 916, loss = 0.00558941\nIteration 917, loss = 0.00554617\nIteration 918, loss = 0.00560878\nIteration 919, loss = 0.00557415\nIteration 920, loss = 0.00568527\nIteration 921, loss = 0.00552902\nIteration 922, loss = 0.00565581\nIteration 923, loss = 0.00560013\nIteration 924, loss = 0.00548109\nIteration 925, loss = 0.00549396\nIteration 926, loss = 0.00553759\nIteration 927, loss = 0.00546304\nIteration 928, loss = 0.00545265\nIteration 929, loss = 0.00544734\nIteration 930, loss = 0.00542364\nIteration 931, loss = 0.00546839\nIteration 932, loss = 0.00539170\nIteration 933, loss = 0.00540486\nIteration 934, loss = 0.00539502\nIteration 935, loss = 0.00548068\nIteration 936, loss = 0.00539332\nIteration 937, loss = 0.00538337\nIteration 938, loss = 0.00548238\nIteration 939, loss = 0.00534180\nIteration 940, loss = 0.00538270\nIteration 941, loss = 0.00530424\nIteration 942, loss = 0.00532893\nIteration 943, loss = 0.00534009\nIteration 944, loss = 0.00530686\nIteration 945, loss = 0.00529386\nIteration 946, loss = 0.00528944\nIteration 947, loss = 0.00528592\nIteration 948, loss = 0.00532366\nIteration 949, loss = 0.00527344\n",
      "Iteration 950, loss = 0.00539893\nIteration 951, loss = 0.00524751\nIteration 952, loss = 0.00535936\nIteration 953, loss = 0.00528257\nIteration 954, loss = 0.00521215\nIteration 955, loss = 0.00523856\nIteration 956, loss = 0.00523027\nIteration 957, loss = 0.00521541\nIteration 958, loss = 0.00515803\nIteration 959, loss = 0.00517978\nIteration 960, loss = 0.00520893\nIteration 961, loss = 0.00517361\nIteration 962, loss = 0.00520989\nIteration 963, loss = 0.00517958\nIteration 964, loss = 0.00520684\nIteration 965, loss = 0.00516031\nIteration 966, loss = 0.00516583\nIteration 967, loss = 0.00521517\nIteration 968, loss = 0.00518868\nIteration 969, loss = 0.00512267\nIteration 970, loss = 0.00509935\nIteration 971, loss = 0.00510331\nIteration 972, loss = 0.00507917\nIteration 973, loss = 0.00511708\nIteration 974, loss = 0.00508292\nIteration 975, loss = 0.00507450\nIteration 976, loss = 0.00507586\nIteration 977, loss = 0.00518015\nIteration 978, loss = 0.00512468\nIteration 979, loss = 0.00522586\nIteration 980, loss = 0.00504887\nIteration 981, loss = 0.00510545\nIteration 982, loss = 0.00508477\nIteration 983, loss = 0.00500433\nIteration 984, loss = 0.00501480\nIteration 985, loss = 0.00500566\nIteration 986, loss = 0.00498692\nIteration 987, loss = 0.00500477\nIteration 988, loss = 0.00497989\nIteration 989, loss = 0.00495175\n",
      "Iteration 990, loss = 0.00498870\nIteration 991, loss = 0.00494807\nIteration 992, loss = 0.00501139\nIteration 993, loss = 0.00495483\nIteration 994, loss = 0.00498815\nIteration 995, loss = 0.00500034\nIteration 996, loss = 0.00488666\nIteration 997, loss = 0.00495100\nIteration 998, loss = 0.00496775\nIteration 999, loss = 0.00490337\nIteration 1000, loss = 0.00495313\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/johnny/anaconda3/envs/redes_neurais/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n  % self.max_iter, ConvergenceWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classificador = MLPClassifier(verbose = True,\n",
    "                              max_iter=1000,\n",
    "                              tol = 0.0000010,\n",
    "                              solver = 'adam',\n",
    "                              hidden_layer_sizes=(100),\n",
    "                              activation='relu')\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "previsoes = classificador.predict(previsores_teste)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "matriz = confusion_matrix(classe_teste, previsoes)\n",
    "\n",
    "print(precisao)\n",
    "print(matriz)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}